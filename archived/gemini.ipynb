{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.352\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\maula\\documents\\project\\chatbot_ftmm\\chatbot_ftmm\\lib\\site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n",
      "---\n",
      "Name: langchain-core\n",
      "Version: 0.1.2\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\maula\\documents\\project\\chatbot_ftmm\\chatbot_ftmm\\lib\\site-packages\n",
      "Requires: anyio, jsonpatch, langsmith, packaging, pydantic, PyYAML, requests, tenacity\n",
      "Required-by: langchain, langchain-community, langchain-experimental, langchain-google-genai\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setting up the Auth\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCoe2MIq7Iy55JMgaGNKAYDydvi4y8EBf4\"\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       top_p=1.0,\n",
       "       top_k=1),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a large language model, trained by Google.\n",
       "\n",
       "I am developed to be informative and comprehensive, to provide a balanced and comprehensive view of all topics, and to generate human-like text in response to a wide range of prompts and questions.\n",
       "\n",
       "Here are some of the things I can do:\n",
       "\n",
       "* **Answer questions:** I can answer questions on a wide range of topics, including science, history, culture, and current events. I can also provide definitions and explanations of concepts.\n",
       "* **Summarize text:** I can summarize text, including articles, news stories, and research papers. I can also extract key points and identify the main arguments of a text.\n",
       "* **Translate languages:** I can translate text between over 100 languages.\n",
       "* **Generate text:** I can generate text, including stories, poems, and scripts. I can also generate code and other technical content.\n",
       "* **Answer in a fun and creative way:** I can be fun and creative in my responses, and I can generate text that is entertaining and engaging.\n",
       "\n",
       "I am still under development, but I am learning and improving every day. I am excited to see what I can achieve in the future.\n",
       "\n",
       "Beyond the above, I can also:\n",
       "\n",
       "* **Write different types of creative content:** I can write songs, poems, short stories, and even movie scripts.\n",
       "* **Generate ideas:** I can help you brainstorm ideas for projects, businesses, or creative endeavors.\n",
       "* **Provide customer service:** I can answer questions, provide information, and resolve issues for customers.\n",
       "* **Help with research:** I can help you find information on a wide range of topics.\n",
       "* **Be a language tutor:** I can help you learn a new language.\n",
       "* **Be a personal assistant:** I can help you with tasks such as scheduling appointments, setting reminders, and taking notes.\n",
       "\n",
       "I can understand and respond to a wide range of questions and requests, and I am always learning new things. If you have a question or a task that you need help with, just ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text\n",
    "prompt = 'Who are you and what can you do?'\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLM stands for Large Language Model. It is a type of artificial intelligence (AI) that has been trained on a massive amount of text data. This training allows LLMs to understand and generate human language in a way that is both coherent and informative.\n",
       "\n",
       "LLMs are used in a variety of applications, including:\n",
       "\n",
       "* **Natural language processing (NLP)**: LLMs can be used to perform a variety of NLP tasks, such as text summarization, machine translation, and question answering.\n",
       "* **Chatbots and virtual assistants**: LLMs can be used to power chatbots and virtual assistants that can interact with humans in a natural way.\n",
       "* **Content generation**: LLMs can be used to generate text, such as articles, blog posts, and marketing copy.\n",
       "* **Code generation**: LLMs can be used to generate code in a variety of programming languages.\n",
       "\n",
       "LLMs are still under development, but they have the potential to revolutionize the way we interact with computers. As LLMs continue to improve, they will be able to perform more complex tasks and be used in even more applications.\n",
       "\n",
       "Here are some of the benefits of using LLMs:\n",
       "\n",
       "* **Accuracy**: LLMs are trained on a massive amount of data, which gives them a deep understanding of human language. This allows them to generate text that is both accurate and informative.\n",
       "* **Coherence**: LLMs are able to generate text that is coherent and flows well. This is because they are trained on text that is written by humans, which gives them a sense of how language should be used.\n",
       "* **Creativity**: LLMs can be used to generate creative text, such as stories, poems, and songs. This is because they are able to learn from the patterns and structures of human language.\n",
       "\n",
       "LLMs are a powerful tool that can be used to improve the way we interact with computers. As they continue to develop, they will be able to perform even more complex tasks and be used in even more applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BaseModel',\n",
       " 'Dict',\n",
       " 'Embeddings',\n",
       " 'Field',\n",
       " 'GoogleGenerativeAIEmbeddings',\n",
       " 'GoogleGenerativeAIError',\n",
       " 'List',\n",
       " 'Optional',\n",
       " 'SecretStr',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'genai',\n",
       " 'get_from_dict_or_env',\n",
       " 'root_validator']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain_google_genai.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_embed',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " 'aembed_documents',\n",
       " 'aembed_query',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'embed_documents',\n",
       " 'embed_query',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_environment']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(GoogleGenerativeAIEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdocsearch\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "docsearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_ftmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
